
Complete implementation of a Variational Autoencoder in Tensorflow and understanding the behavior using MNIST dataset.

The idea of this mini-project is to have some hands-on and gain some familiarity with the generative modeling techniques. VAE will try to understand and learn the underlying distributions from the data, and once successful, it will be then able to generate data itself. 

![alt text](https://github.com/kumarnikhil936/VariationalAutoencoderOnMNIST/blob/master/regenerated_images.png)

In the figure above, we can see that the network is able to generate the new images based on the knowledge it has achieved during training. These images are completely new and the network has never seen them before during training. 

We can also try to see how based on the input provided to the decoder network, the network generates the new output image and what sort of transition is there. In the figure below, we can see how image of one label transitions to the other one. 

![alt text](https://github.com/kumarnikhil936/VariationalAutoencoderOnMNIST/blob/master/transition_image.png)


Since we had kept the bottleneck layer width as only 2, we can easily show the distribbution it has learnt in the encodings. The different labels are color coded accordingly. 
![alt text](https://github.com/kumarnikhil936/VariationalAutoencoderOnMNIST/blob/master/latent_plot.png)


Unlike normal autoencoders, the values in the latent layer of the variational autoencoders are not deterministically generated by the encoder. Instead, the encoder will generate the parameters defining random variables, which follow generally independent Gaussing distributions and the encoder generates a vector with a mean and standard deviation of the Gaussians. So, it might or might not be a GoTo option based on the type of application one might consider it for.
